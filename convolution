Convolution:

當一個Function輸入一個系統(也是Function)後, 會輸出為另一個Function. 而, 該輸出的Function為每個輸入訊號所產生的當下效應及殘留效應. 為了表示這樣的關係, 及將其稱呼為Convolue.

https://stackoverflow.com/questions/17337332/why-convolution-is-required-or-what-is-the-philosophy-behind-convolution
If you refer to the wikipedia article regarding LTI system theory you will see that the impulse response h(t) dictates how the system responds to an impulse d(t). We also know that the equation for the output of a system is y(t) = h(t) * x(t) in the time domain. Where y(t) is the output, h(t) is the impulse response, x(t) is the input. The symbol * refers to the convolution of h(t) and x(t)

In the frequency domain (by performing the Laplace transform), we know that Y(s) = H(s)X(s), thus the output response to the input X(s) is the Multiplication of the transfer function H(s) and the input X(s).


兩個Function做Convolue, 有兩種表示方法:
http://blog.csdn.net/bitcarmanlee/article/details/54729807
1. 
x[0, 1, 2]分別去乘y[0], 落在axis為0, 1, 2
x[0, 1, 2]分別去乘y[1], 落在axis為1, 2, 3
x[0, 1, 2]分別去乘y[2], 落在axis為2, 3, 4
在axis去做各點相加, 其結果即為Convolution
以axis為0來說, 那時, 只有x[0]和y[0]的乘積
但, 到了axis為1, 則除了有x[1]和y[0], 也有x[0]和y[1]. 其x[1]和y[0], 是新輸入的乘積. 其x[0]和y[1], 則是之前x[0]在系統內的殘留效應, 而效應的大小是y[1].
以此法, 則是一次各別算, 當x[0]時, 和y[n]的各別乘績. 當x[1]時, 和y[n]的各別乘績. 但, 如要知道axis 0, 1, 2的結果, 則需再相加. 從圖示上, 也較難看出, 在axis 0, 1, 2時的各別效應.

2. 
http://blog.csdn.net/zyex1108/article/details/45397637
另外一種, 則是Reflect再相乘. 差別在於, 透過這種方式, 可以一次算出系統分別在axis 0, 1, or 2的各別所有反應.
也就是說, 系統function轉折, 
看axis 0時, 剛好會是x[0]和y[0]的乘積
看axis 1時, 剛好會是x[0]和y[1]的乘積, 和x[1]和y[0]的乘積. 
以此法, 可以在圖示看到, 當axis 0時, x[0]和y[0]有乘積, 在axis 1時, x[0]和y[1]有overlay, x[1]和y[0]有overaly, 也就是在axis 0時, 系統的output是要考慮兩者的影響.

https://www.quora.com/How-can-I-explain-in-a-simple-manner-what-convolution-is-and-why-it-is-important/answer/Mahesh-C-Shastry
https://www.quora.com/Digital-Signal-Processing/How-can-I-explain-in-a-simple-manner-what-convolution-is-and-why-it-is-important/answer/Mahesh-C-Shastry?share=1&srid=pd9d
https://www.quora.com/Digital-Signal-Processing/How-can-I-explain-in-a-simple-manner-what-convolution-is-and-why-it-is-important/answer/Raeed-Chowdhury?share=1&srid=hj0Y
https://www.quora.com/Digital-Signal-Processing-How-do-you-explain-without-doing-any-calculation-that-convolution-in-time-domain-should-imply-multiplication-in-frequency-domain
多項式的Convolue, 即為多項式的係數相乘

https://www.quora.com/How-can-I-explain-in-a-simple-manner-what-convolution-is-and-why-it-is-important

Let me put it without much math. Consider multiplication: As and when you get a number, you multiply it by one more number and pass it off as output. A multiplier does not remember stuff. Its like a small child. It forgets all the things you did in the past and responds to the current input. If you smile, it smiles back, so forth. 
But, a convolution is different. It remembers your past actions. Based on that and your present doing, it will decide what to do. If you smile at it and you did real-bad before, it would not smile back at you! 

https://www.quora.com/How-can-I-explain-in-a-simple-manner-what-convolution-is-and-why-it-is-important
Raeed Chowdhury, Electrical Engineer
它看起來不同的原因是因為在這種情況下，你正在找到每個輸入信號的整個輸出信號的貢獻。但, Convoluation是看某個特定時間點


https://www.quora.com/Digital-Signal-Processing-How-do-you-explain-without-doing-any-calculation-that-convolution-in-time-domain-should-imply-multiplication-in-frequency-domain


https://www.quora.com/What-is-an-intuitive-way-of-explaining-how-the-Fourier-transform-works/answer/Mark-Eichenlaub


https://www.quora.com/Why-do-CNNs-use-convolution-instead-of-cross-correlation
Actually most practical applications of convolutional neural networks (CNN) use cross-correlation instead of convolutions.

Convolution operation either flips the source image or the kernel weights. Flipping actually adds some unnecessary complexity to the CNN code especially that the flipping operation occurs during inference and backpropagation of errors.
On the other hand cross-correlation does not flip the source image or kernel weights. This results in simplified CNN code. Thus cross-correlation is more convenient to implement in CNNs than convolution operation itself.

You can call them that if you want to, but in CNN terms the two are the same. You can do a flip or not, the CNN will just adapt accordingly. If you do a flip then the CNN just learns flipped weights as well so it technically doesn't change anything.


單位脈衝的系統響應會等於它自己
而, 任何的信號都可以拆解為一個個的單位脈衝訊號
因此, 我們將輸入訊號拆解成脈衝訊號的線性關係(加, 減, 乘, 除), 再分別將其對應的脈衝響應作線性組合, 即可得到該輸入訊號的系統響應
在時域, 將輸入訊號拆解成脈衝訊號的線性關係, 再將其對應的脈衝響應以積的方式表示出整體輸出


https://dsp.stackexchange.com/questions/4723/what-is-the-physical-meaning-of-the-convolution-of-two-signals
The input-output behavior of an LTI system can be characterized via its impulse response, and the output of an LTI system for any input signal x(t)x(t) can be expressed as the convolution of the input signal with the system's impulse response.


https://www.quora.com/Digital-Signal-Processing-How-do-you-explain-without-doing-any-calculation-that-convolution-in-time-domain-should-imply-multiplication-in-frequency-domain

http://cc.ee.ntu.edu.tw/~fengli/Teaching/SignalsSystems/
